{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95046339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import tqdm\n",
    "from pyvirtualdisplay import Display\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f690cf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f10a5a02550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = Display(visible=0, size=(1920,1080))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6131f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/harong/chromedriver'\n",
    "driver = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd63373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 무조건 로그인 해야함. \n",
    "driver.get('https://nid.naver.com/nidlogin.login?svctype=262144&url=http://naver.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6a5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://cafe.naver.com/mbticafe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c17553",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"menuLink0\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136c2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_mbti_data() :\n",
    "    tmp_li = dict()\n",
    "    for i in tqdm.tqdm(range(602268,0,-1)) : \n",
    "        pg = str(i)\n",
    "        addr = \"https://cafe.naver.com/ArticleRead.nhn?clubid=11856775&boardtype=L&articleid=\"+str(i)+\"&referrerAllArticles=true\"\n",
    "        driver.get(addr)\n",
    "        try : \n",
    "            result = driver.switch_to_alert()\n",
    "            result.accept()\n",
    "            result.dismiss()\n",
    "        except : \n",
    "            pass\n",
    "        driver.switch_to.frame('cafe_main')\n",
    "        html = driver.page_source\n",
    "        soup = bs(html, 'html.parser')\n",
    "        writer = soup.findAll(\"a\",{\"class\":\"m-tcol-c b\"}).text.split('(')[0]\n",
    "        category = soup.findAll(\"a\", {\"class\": \"m-tcol-c\"})[3].text\n",
    "        title = soup.find(\"span\",{\"class\":\"b m-tcol-c\"}).text\n",
    "        article = soup.findAll(\"div\", {\"class\": \"tbody m-tcol-c\"})[0].text.replace('\\u200b', ' ').replace('\\n','').strip()\n",
    "\n",
    "        tmp_li[i] = {'writer':writer,'category':category,'title':title,'article':article}\n",
    "        \n",
    "        if i % 100 == 0 :\n",
    "            with open('/home/harong/main/dev/crawl_data/crawl_res_{i}.json','w') as f:\n",
    "                json.dump(tmp_li,f)\n",
    "            f.close()\n",
    "            tmp_li = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e6b2f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                | 0/602268 [00:00<?, ?it/s]/home/harong/anaconda3/envs/mbti/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: use driver.switch_to.alert instead\n",
      "  \n",
      "  0%|                                                                                                                                | 0/602268 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_620/2552787770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawl_mbti_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_620/2408014735.py\u001b[0m in \u001b[0;36mcrawl_mbti_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"m-tcol-c b\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"m-tcol-c\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"b m-tcol-c\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mbti/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2252\u001b[0m         \u001b[0;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m         raise AttributeError(\n\u001b[0;32m-> 2254\u001b[0;31m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2255\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "crawl_mbti_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5a702de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_mbti_data():\n",
    "    pg = str(0)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(1, 1000)):\n",
    "        pg = str(i)\n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_category = []\n",
    "        tmp_writer = []\n",
    "        tmp_title = []\n",
    "        tmp_link = []\n",
    "        \n",
    "        addr = \"https://cafe.naver.com/ArticleList.nhn?search.clubid=11856775&userDisplay=50&search.boardtype=L&search.specialmenutype=&search.totalCount=501&search.cafeId=11856775&search.page=\" + pg            \n",
    "\n",
    "        driver.get(addr)\n",
    "        driver.switch_to.frame('cafe_main')\n",
    "        \n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = bs(html, 'html.parser')\n",
    "        \n",
    "        a_category_list = soup.findAll(\"div\", {\"class\": \"inner_name\"})\n",
    "        a_title_list = soup.findAll(\"a\",{\"class\":\"article\"})\n",
    "        a_writer_list = soup.findAll(\"a\",{\"class\":\"m-tcol-c\"}) \n",
    "        \n",
    "        if pg == '1':\n",
    "            a_title_list = a_title_list[16:]\n",
    "            a_writer_list = a_writer_list[16:]\n",
    "            \n",
    "        for j in range(0, len(a_title_list)):\n",
    "            tmp_writer.append(a_writer_list[j].text.strip())\n",
    "            tmp_title.append(a_title_list[j].text.strip())\n",
    "            tmp_category.append(a_category_list[j].text.strip())\n",
    "            tmp_link.append(\"https://cafe.naver.com/mbticafe\" + a_title_list[j].get('href').strip())\n",
    "        \n",
    "        tmp_df['category'] = tmp_category\n",
    "        tmp_df['title'] = tmp_title\n",
    "        tmp_df['writer'] = tmp_writer\n",
    "        tmp_df['link'] = tmp_link\n",
    "        \n",
    "        tmp_df.to_csv(f'/home/harong/main/dev/crawl_data/crawl_res_{i}.csv', index=False)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1af916c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 999/999 [20:54<00:00,  1.2\n"
     ]
    }
   ],
   "source": [
    "crawl_mbti_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99f58427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [] None []\n"
     ]
    }
   ],
   "source": [
    "addr = \"https://cafe.naver.com/ArticleRead.nhn?clubid=11856775&boardtype=L&articleid=602225&referrerAllArticles=true\"\n",
    "driver.get(addr)\n",
    "# driver.switch_to.frame('cafe_main')\n",
    "html = driver.page_source\n",
    "soup = bs(html, \"html.parser\")\n",
    "# category = soup.findAll(\"a\", {\"class\": \"link_board\"})\n",
    "# title = soup.findAll(\"h3\",{\"class\":\"title_text\"})\n",
    "writer = soup.find(\"div\",{\"Writer_Info\"})\n",
    "# article = soup.findAll(\"div\", {\"class\": \"tbody m-tcol-c\"})\n",
    "print(category,title,writer,article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02904cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_article(path):\n",
    "    file_list = os.listdir(path)\n",
    "    for file in tqdm.tqdm(file_list):\n",
    "        file_num = file.split('.')[0].split('_')[-1]\n",
    "        df = pd.read_csv(path + '/' + file)\n",
    "        tmp_df = pd.DataFrame()\n",
    "        \n",
    "        row_len = len(df['category'])\n",
    "        tmp_article = []\n",
    "        tmp_category = []\n",
    "        tmp_writer = []\n",
    "        tmp_title = []\n",
    "        tmp_link = []\n",
    "        for r_idx in range(0, row_len):\n",
    "            try:\n",
    "                row = df.iloc[r_idx]\n",
    "                if row['category'] == '가입 인사 (등업 필수)':\n",
    "                    continue\n",
    "\n",
    "                driver.get(row['link'])\n",
    "                driver.switch_to.frame('cafe_main')\n",
    "                html = driver.page_source\n",
    "                soup = bs(html, 'html.parser')\n",
    "                article = soup.findAll(\"div\", {\"class\": \"tbody m-tcol-c\"})[0].text.replace('\\u200b', ' ').replace('\\n','').strip()\n",
    "                if article.strip() == '':\n",
    "                    continue\n",
    "\n",
    "                tmp_article.append(article)\n",
    "                tmp_category.append(row['category'])\n",
    "                tmp_writer.append(row['writer'])\n",
    "                tmp_link.append(row['link'])\n",
    "                tmp_title.append(row['title'])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        tmp_df['category'] = tmp_category\n",
    "        tmp_df['title'] = tmp_title\n",
    "        tmp_df['writer'] = tmp_writer\n",
    "        tmp_df['link'] = tmp_link\n",
    "        tmp_df['article'] = tmp_article\n",
    "        \n",
    "        tmp_df.to_csv(f'/home/harong/main/dev/crawl_data/crawl_res_{file_num}_fin.csv', index=False)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a178f8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "path = '/home/harong/main/dev/crawl_data/article'\n",
    "crawl_article(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9fc77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ea59ec9597b25502e1f1a3fe8c2f99bc3eeffee07fe77426fc9470dcd9fe6b9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
